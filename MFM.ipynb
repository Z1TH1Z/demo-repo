{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Z1TH1Z/demo-repo/blob/main/MFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtAyv9LzdwQr"
      },
      "source": [
        "# **Import necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.stats import skew, kurtosis"
      ],
      "metadata": {
        "id": "wYfA7Jl2RrU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNowxPoUd5Oc"
      },
      "source": [
        "# **Neural Network-based recommender class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZfRUydAddWc"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworkModelRecommender:\n",
        "  # Initialize basic variables and model containers\n",
        "  def __init__(self, epochs=50, batch_size=32, data_type='tabular'):\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.data_type = data_type\n",
        "    self.scaler = StandardScaler()\n",
        "    self.label_encoder = LabelEncoder()\n",
        "    self.ml_models = {\n",
        "      'classifiers': {\n",
        "        'Random Forest Classifier': RandomForestClassifier(),\n",
        "        'Gradient Boosting Classifier': GradientBoostingClassifier(),\n",
        "        'SVM Classifier': SVC(probability=True),\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000)\n",
        "      },\n",
        "      'regressors': {\n",
        "        'Random Forest Regressor': RandomForestRegressor(),\n",
        "        'Gradient Boosting Regressor': GradientBoostingRegressor(),\n",
        "        'SVM Regressor': SVR(),\n",
        "        'Ridge Regression': Ridge()\n",
        "      }\n",
        "    }\n",
        "    self.metadata_model = None      # Neural network to predict best model\n",
        "    self.trained_model_dict = None  # Trained models as references\n",
        "\n",
        "  # Fit and extract features from input data based on its type\n",
        "  def fit(self, data):\n",
        "    if self.data_type == 'tabular':\n",
        "      if hasattr(data, \"iloc\"):\n",
        "        X, y = data.iloc[:, :-1].values, data.iloc[:, -1].values\n",
        "\n",
        "      elif isinstance(data, (tuple, list)) and len(data) == 2:\n",
        "        X, y = data\n",
        "\n",
        "      else:\n",
        "        data = np.array(data)\n",
        "        X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "      if X.ndim > 2:\n",
        "        X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "      X = self.scaler.fit_transform(X)\n",
        "      self.X, self.y = X, y\n",
        "\n",
        "    elif self.data_type == 'image':\n",
        "      # Preprocess image data and extract features via CNN + PCA\n",
        "      X, y = data\n",
        "      X = X.astype('float32') / 255.0\n",
        "      X, y = shuffle(X, y)\n",
        "      sample_size = min(1000, len(X))   # Sample subset for quicker computation\n",
        "      X = X[:sample_size]\n",
        "      y = y[:sample_size]\n",
        "\n",
        "      input_shape = X.shape[1:]\n",
        "      feature_extractor = keras.Sequential([\n",
        "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        keras.layers.Flatten(),\n",
        "      ])\n",
        "      X = feature_extractor.predict(X, verbose=0)\n",
        "\n",
        "      pca = PCA(n_components=min(50, X.shape[1]))\n",
        "      X = pca.fit_transform(X)\n",
        "\n",
        "    elif self.data_type == 'pixel':\n",
        "      # Flatten pixel data and scale\n",
        "      X, y = data\n",
        "      X = X.astype('float32') / 255.0\n",
        "      X = X.reshape(X.shape[0], -1)\n",
        "      X = self.scaler.fit_transform(X)\n",
        "\n",
        "    elif self.data_type == 'text':\n",
        "      # Convert text to TF-IDF features\n",
        "      X, y = data\n",
        "      self.vectorizer = TfidfVectorizer(max_features=1000)\n",
        "      X = self.vectorizer.fit_transform(X).toarray()\n",
        "\n",
        "      if X.ndim > 2:\n",
        "        X = X.reshape(X.shape[0], -1)\n",
        "\n",
        "      X = self.scaler.fit_transform(X)\n",
        "\n",
        "    else:\n",
        "      raise ValueError(\"Invalid data_type. Use 'tabular', 'image', or 'pixel'.\")\n",
        "\n",
        "    # Label encoding the target\n",
        "    y = self.label_encoder.fit_transform(y)\n",
        "\n",
        "    # Extract metadata to help in the meta-model learning process\n",
        "    metadata = self._extract_metadata(X, y)\n",
        "    problem_type = self._infer_problem_type(y)\n",
        "    model_dict = self.ml_models[problem_type]\n",
        "    self.trained_model_dict = model_dict\n",
        "\n",
        "    X_meta, y_meta = [], []\n",
        "\n",
        "    for i, (model_name, model) in enumerate(model_dict.items()):\n",
        "      try:\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "        model.fit(X_train, y_train)\n",
        "        score = model.score(X_test, y_test)\n",
        "        X_meta.append(metadata)\n",
        "        y_meta.append(i)\n",
        "      except Exception as e:\n",
        "        if len(np.unique(y)) < 2:\n",
        "          print(f\"Skipping {model_name}: only one class present in the data\")\n",
        "          continue\n",
        "\n",
        "    X_meta = np.array(X_meta)\n",
        "    y_meta = keras.utils.to_categorical(y_meta, num_classes=len(model_dict))\n",
        "\n",
        "    input_size = X_meta.shape[1]\n",
        "    self.metadata_model = self._build_model(output_size=len(model_dict), input_size=input_size)\n",
        "    self.metadata_model.fit(X_meta, y_meta, epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
        "\n",
        "  # Simple feedforward neural network to predict best model\n",
        "  def _build_model(self, output_size, input_size=None):\n",
        "    if input_size is None:\n",
        "        raise ValueError(\"input_size must be provided to build metadata model.\")\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=(input_size,)),\n",
        "        keras.layers.Dense(64, activation='relu'),\n",
        "        keras.layers.Dense(output_size, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "  # Check for linearly separable data (PCA + Logistic Regression)\n",
        "  def _is_linearly_separable(self, X, y, threshold=0.8):\n",
        "    try:\n",
        "      pca = PCA(n_components=2)\n",
        "      X_reduced = pca.fit_transform(X)\n",
        "      clf = LogisticRegression().fit(X_reduced, y)\n",
        "      return clf.score(X_reduced, y) > threshold\n",
        "    except Exception as e:\n",
        "      print(f\"Linear separable check failed: {e}\")\n",
        "      return False\n",
        "\n",
        "  # Check for linear regression trend (PCA + Ridge Regression)\n",
        "  def _has_linear_trend(self, X, y, threshold=0.8):\n",
        "    try:\n",
        "      pca = PCA(n_components=1)\n",
        "      X_reduced = pca.fit_transform(X)\n",
        "      ridge = Ridge()\n",
        "      ridge.fit(X_reduced, y)\n",
        "      r2_score = ridge.score(X_reduced, y)\n",
        "      return r2_score > threshold\n",
        "    except Exception as e:\n",
        "      print(f\"Linear trend check failed: {e}\")\n",
        "      return False\n",
        "\n",
        "  # Decide problem type (Classification or Regression)\n",
        "  def _infer_problem_type(self, y):\n",
        "    y = np.array(y)\n",
        "\n",
        "    if all(isinstance(item, str) for item in y):\n",
        "      return 'classifiers'\n",
        "\n",
        "    if len(np.unique(y)) <= 20 and (np.issubdtype(y.dtype, np.integer) or np.allclose(y, np.round(y))):\n",
        "      return 'classifiers'\n",
        "\n",
        "    elif np.issubdtype(y.dtype, np.integer) or not np.issubdtype(y.dtype, np.number):\n",
        "      return 'classifiers'\n",
        "\n",
        "    return 'regressors'\n",
        "\n",
        "  # Extract meta features from dataset\n",
        "  def _extract_metadata(self, X, y):\n",
        "    y = np.array(y)\n",
        "    features = [\n",
        "      X.shape[0],\n",
        "      X.shape[1],\n",
        "      len(np.unique(y)),\n",
        "      np.mean(X),\n",
        "      np.std(X),\n",
        "      np.min(X),\n",
        "      np.max(X),\n",
        "      np.median(X),\n",
        "      skew(X.flatten()),\n",
        "      kurtosis(X.flatten()),\n",
        "      1 if np.issubdtype(y.dtype, np.floating) else 0,\n",
        "      1 if np.issubdtype(y.dtype, np.integer) else 0\n",
        "    ]\n",
        "    return np.array(features).reshape(-1)\n",
        "\n",
        "  # Extract text features from the sequence of texts/strings\n",
        "  def _extract_text_features(self, texts, max_len=100):\n",
        "    tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
        "        keras.layers.GlobalAveragePooling1D()\n",
        "    ])\n",
        "    features = model.predict(padded, verbose=0)\n",
        "    return features\n",
        "\n",
        "  # Choose suitable evaluation metric\n",
        "  def _select_metrics(self, y):\n",
        "    if self._infer_problem_type(y) == 'classifiers':\n",
        "      return 'accuracy'\n",
        "    elif np.issubdtype(y.dtype, np.floating):\n",
        "      return 'r2'\n",
        "    else:\n",
        "      return 'neg_mean_absolute_error'\n",
        "\n",
        "  # Recommend based on data type\n",
        "  def recommend(self, data):\n",
        "    if self.data_type == 'image':\n",
        "      return self._recommend_image(data)\n",
        "    elif self.data_type == 'pixel':\n",
        "      return self._recommend_pixel(data)\n",
        "    elif self.data_type == 'tabular':\n",
        "      return self._recommend_tabular(data)\n",
        "    elif self.data_type == 'text':\n",
        "      return self._recommend_text(data)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid data type. Supported types are 'image', 'pixel', 'tabular', and 'text'.\")\n",
        "\n",
        "  # Recommendation function if data type is 'image'\n",
        "  def _recommend_image(self, data):\n",
        "    X, y = data\n",
        "    X = X.astype('float32') / 255.0\n",
        "    problem_type = self._infer_problem_type(y)\n",
        "    model_dict = self.ml_models[problem_type]\n",
        "    scoring_metric = self._select_metrics(y)\n",
        "\n",
        "    input_shape = X.shape[1:]\n",
        "    feature_extractor = keras.Sequential([\n",
        "      keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "      keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "      keras.layers.Flatten(),\n",
        "    ])\n",
        "\n",
        "    X_features = feature_extractor.predict(X)\n",
        "\n",
        "    if problem_type == 'classifiers' and self._is_linearly_separable(X_features, y):\n",
        "      model = model_dict['Logistic Regression']\n",
        "      score = cross_val_score(model, X_features, y, cv=5, scoring=scoring_metric)\n",
        "      return 'classifiers', 'Logistic Regression', scoring_metric, float(np.mean(score))\n",
        "\n",
        "    if self.metadata_model is None:\n",
        "      raise ValueError(\"Model has not been trained. Please call `.fit()` first.\")\n",
        "\n",
        "    metadata = self._extract_metadata(X_features, y).reshape(1, -1)\n",
        "    prediction = self.metadata_model.predict(metadata.reshape(1, -1)).flatten()\n",
        "    best_model_idx = np.argmax(prediction)\n",
        "    best_model_name = list(model_dict.keys())[best_model_idx]\n",
        "    best_model = model_dict[best_model_name]\n",
        "\n",
        "    scores = cross_val_score(best_model, X_features, y, cv=5, scoring=scoring_metric)\n",
        "    return problem_type, best_model_name, scoring_metric, float(np.mean(scores))\n",
        "\n",
        "  # Recommendation function if data type is 'pixel'\n",
        "  def _recommend_pixel(self, data):\n",
        "    X, y = data\n",
        "    X = X.astype('float32') / 255.0\n",
        "    problem_type = self._infer_problem_type(y)\n",
        "    model_dict = self.ml_models[problem_type]\n",
        "    scoring_metric = self._select_metrics(y)\n",
        "\n",
        "    X_flat = X.reshape(X.shape[0], -1)\n",
        "    X_flat = self.scaler.fit_transform(X_flat)\n",
        "\n",
        "    if problem_type == 'classifiers' and self._is_linearly_separable(X_flat, y):\n",
        "      model = model_dict['Logistic Regression']\n",
        "      score = cross_val_score(model, X_flat, y, cv=5, scoring=scoring_metric)\n",
        "      return 'classifiers', 'Logistic Regression', scoring_metric, float(np.mean(score))\n",
        "\n",
        "    if problem_type == 'regressors' and self._has_linear_trend(X_flat, y):\n",
        "      model = model_dict['Ridge Regression']\n",
        "      score = cross_val_score(model, X_flat, y, cv=5, scoring=scoring_metric)\n",
        "      return 'regressors', 'Ridge Regression', scoring_metric, float(np.mean(score))\n",
        "\n",
        "    if self.metadata_model is None:\n",
        "      raise ValueError(\"Model has not been trained. Please call `.fit()` first.\")\n",
        "\n",
        "    metadata = self._extract_metadata(X_flat, y).reshape(1, -1)\n",
        "    prediction = self.metadata_model.predict(metadata).flatten()\n",
        "    best_model_idx = np.argmax(prediction)\n",
        "    best_model_name = list(model_dict.keys())[best_model_idx]\n",
        "    best_model = model_dict[best_model_name]\n",
        "\n",
        "    scores = cross_val_score(best_model, X_flat, y, cv=5, scoring=scoring_metric)\n",
        "    return problem_type, best_model_name, scoring_metric, float(np.mean(scores))\n",
        "\n",
        "  # Recommendation function if data type is 'tabular'\n",
        "  def _recommend_tabular(self, data):\n",
        "    if hasattr(data, \"iloc\"):\n",
        "      X, y = data.iloc[:, :-1].values, data.iloc[:, -1].values\n",
        "\n",
        "    elif isinstance(data, (tuple, list)) and len(data) == 2:\n",
        "      X, y = data\n",
        "\n",
        "    else:\n",
        "      data = np.array(data)\n",
        "      X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "    X = self.scaler.fit_transform(X)\n",
        "\n",
        "    metadata = self._extract_metadata(X, y).reshape(1, -1)\n",
        "\n",
        "    problem_type = self._infer_problem_type(y)\n",
        "    model_dict = self.ml_models[problem_type]\n",
        "    scoring_metric = self._select_metrics(y)\n",
        "\n",
        "    if problem_type == 'classifiers' and self._is_linearly_separable(X, y):\n",
        "      if 'Logistic Regression' in model_dict:\n",
        "        model = model_dict['Logistic Regression']\n",
        "        score = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)\n",
        "        metric_value = -np.mean(score) if scoring_metric.startswith('neg_') else np.mean(score)\n",
        "        return 'classifiers', 'Logistic Regression', scoring_metric, float(metric_value)\n",
        "\n",
        "    if problem_type == 'regressors' and self._has_linear_trend(X, y):\n",
        "      if 'Ridge Regression' in model_dict:\n",
        "        model = model_dict['Ridge Regression']\n",
        "        score = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)\n",
        "        metric_value = -np.mean(score) if scoring_metric.startswith('neg_') else np.mean(score)\n",
        "        return 'regressors', 'Ridge Regression', scoring_metric, float(metric_value)\n",
        "\n",
        "    if self.metadata_model is None:\n",
        "      raise ValueError(\"Model has not been trained. Please call `.fit()` first.\")\n",
        "\n",
        "    prediction = self.metadata_model.predict(metadata.reshape(1, -1)).flatten()\n",
        "    best_model_idx = np.argmax(prediction)\n",
        "    best_model_name = list(model_dict.keys())[best_model_idx]\n",
        "    best_model = model_dict[best_model_name]\n",
        "\n",
        "    scores = cross_val_score(best_model, X, y, cv=5, scoring=scoring_metric)\n",
        "    metric_value = -np.mean(scores) if scoring_metric.startswith('neg_') else np.mean(scores)\n",
        "\n",
        "    return problem_type, best_model_name, scoring_metric, float(metric_value)\n",
        "\n",
        "  # Recommendation function if data type is 'text'\n",
        "  def _recommend_text(self, data):\n",
        "    X_raw, y = data\n",
        "\n",
        "    if not hasattr(self, 'vectorizer'):\n",
        "      raise ValueError(\"Text vectorizer not found. Please call `.fit()` first.\")\n",
        "\n",
        "    X = self.vectorizer.transform(X_raw).toarray()\n",
        "    X = self.scaler.transform(X)\n",
        "\n",
        "    problem_type = self._infer_problem_type(y)\n",
        "    model_dict = self.ml_models[problem_type]\n",
        "    scoring_metric = self._select_metrics(y)\n",
        "\n",
        "    if problem_type == 'classifiers' and self._is_linearly_separable(X, y):\n",
        "      model = model_dict['Logistic Regression']\n",
        "      score = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)\n",
        "      return 'classifiers', 'Logistic Regression', scoring_metric, float(np.mean(score))\n",
        "\n",
        "    if problem_type == 'regressors' and self._has_linear_trend(X, y):\n",
        "      model = model_dict['Ridge Regression']\n",
        "      score = cross_val_score(model, X, y, cv=5, scoring=scoring_metric)\n",
        "      return 'regressors', 'Ridge Regression', scoring_metric, float(np.mean(score))\n",
        "\n",
        "    if self.metadata_model is None:\n",
        "      raise ValueError(\"Model has not been trained. Please call `.fit()` first.\")\n",
        "\n",
        "    metadata = self._extract_metadata(X, y).reshape(1, -1)\n",
        "    prediction = self.metadata_model.predict(metadata).flatten()\n",
        "    best_model_idx = np.argmax(prediction)\n",
        "    best_model_name = list(model_dict.keys())[best_model_idx]\n",
        "    best_model = model_dict[best_model_name]\n",
        "\n",
        "\n",
        "    scores = cross_val_score(best_model, X, y, cv=5, scoring=scoring_metric)\n",
        "    return problem_type, best_model_name, scoring_metric, float(np.mean(scores))\n",
        "\n",
        "  # Save the trained metadata neural model\n",
        "  def save_model(self, filepath='metadata_model.h5'):\n",
        "    if self.metadata_model:\n",
        "        self.metadata_model.save(filepath)\n",
        "\n",
        "  # Load the saved neural recommender\n",
        "  def load_model(self, filepath='metadata_model.h5'):\n",
        "      self.metadata_model = keras.models.load_model(filepath)\n",
        "\n",
        "  # Evaluate the overall performance of the neural model\n",
        "  def evaluate_recommendation_accuracy(self, data):\n",
        "    if self.metadata_model is None:\n",
        "        raise ValueError(\"Please train the model using `.fit()` before evaluation.\")\n",
        "\n",
        "    if self.data_type == 'tabular':\n",
        "        if hasattr(data, \"iloc\"):\n",
        "            X, y = data.iloc[:, :-1].values, data.iloc[:, -1].values\n",
        "        elif isinstance(data, (tuple, list)) and len(data) == 2:\n",
        "            X, y = data\n",
        "        else:\n",
        "            data = np.array(data)\n",
        "            X, y = data[:, :-1], data[:, -1]\n",
        "\n",
        "        X = self.scaler.fit_transform(X)\n",
        "\n",
        "    elif self.data_type == 'pixel':\n",
        "        X, y = data\n",
        "        X = X.astype('float32') / 255.0\n",
        "        X = X.reshape(X.shape[0], -1)\n",
        "        X = self.scaler.fit_transform(X)\n",
        "\n",
        "    elif self.data_type == 'text':\n",
        "        X_raw, y = data\n",
        "        X = self.vectorizer.transform(X_raw).toarray()\n",
        "        X = self.scaler.transform(X)\n",
        "\n",
        "    elif self.data_type == 'image':\n",
        "        X, y = data\n",
        "        X = X.astype('float32') / 255.0\n",
        "        input_shape = X.shape[1:]\n",
        "        feature_extractor = keras.Sequential([\n",
        "            keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
        "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            keras.layers.Flatten(),\n",
        "        ])\n",
        "        X = feature_extractor.predict(X, verbose=0)\n",
        "        pca = PCA(n_components=min(50, X.shape[1]))\n",
        "        X = pca.fit_transform(X)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported data_type for evaluation.\")\n",
        "\n",
        "    y = self.label_encoder.fit_transform(y)\n",
        "    metadata = self._extract_metadata(X, y).reshape(1, -1)\n",
        "    problem_type = self._infer_problem_type(y)\n",
        "    model_dict = self.ml_models[problem_type]\n",
        "    scoring_metric = self._select_metrics(y)\n",
        "\n",
        "    # Get actual model performances\n",
        "    model_scores = {}\n",
        "    for model_name, model in model_dict.items():\n",
        "        try:\n",
        "            score = cross_val_score(model, X, y, cv=3, scoring=scoring_metric)\n",
        "            model_scores[model_name] = np.mean(score)\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    if not model_scores:\n",
        "        return 0.0\n",
        "\n",
        "    # Normalize model scores\n",
        "    scores = np.array(list(model_scores.values()))\n",
        "    min_score = scores.min()\n",
        "    max_score = scores.max()\n",
        "    if max_score == min_score:\n",
        "        norm_scores = {k: 1.0 for k in model_scores}  # If all models same score\n",
        "    else:\n",
        "        norm_scores = {k: (v - min_score) / (max_score - min_score) for k, v in model_scores.items()}\n",
        "\n",
        "    # Predicted best model by metadata model\n",
        "    prediction = self.metadata_model.predict(metadata, verbose=0).flatten()\n",
        "    predicted_model_idx = np.argmax(prediction)\n",
        "    predicted_model_name = list(model_dict.keys())[predicted_model_idx]\n",
        "\n",
        "    return norm_scores.get(predicted_model_name, 0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgqpFLPedoSq"
      },
      "source": [
        "# **Training and Testing**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on Pixel Dataset\n"
      ],
      "metadata": {
        "id": "OxRwB3XGBW-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST data\n",
        "(X_train, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "# Reshape to include channel dimension\n",
        "X_train = X_train.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Take a small sample\n",
        "X_sample = X_train[:1000]\n",
        "y_sample = y_train[:1000]\n",
        "\n",
        "# Instantiate and train the recommender\n",
        "recommender = NeuralNetworkModelRecommender(data_type='pixel')\n",
        "recommender.fit((X_sample, y_sample))\n",
        "\n",
        "# Get model recommendation\n",
        "result = recommender.recommend((X_sample, y_sample))\n",
        "print(\"\\nRecommendation Result:\")\n",
        "print(f\"Problem Type: {result[0]}\")\n",
        "print(f\"Model Name: {result[1]}\")\n",
        "print(f\"Scoring Metric: {result[2]}\")\n",
        "print(f\"Score: {result[3]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "Gy00tR_Cu5-G",
        "outputId": "698f1e0a-485c-4c8b-e7d7-062f7e54497b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a89f9d1cc9ab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load MNIST data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "accuracy_score = recommender.evaluate_recommendation_accuracy((\n",
        "    X_sample[:200],\n",
        "    y_sample[:200]\n",
        "))\n",
        "print(f\"Recommendation Accuracy (probabilistic): {accuracy_score:.4f}\")"
      ],
      "metadata": {
        "id": "UMKqlCjrTKtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on Text Dataset"
      ],
      "metadata": {
        "id": "c7m93TrB3aOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare text data and labels\n",
        "texts = [\n",
        "    \"I love this product!\",\n",
        "    \"This is the worst thing ever.\",\n",
        "    \"Amazing experience, would buy again.\",\n",
        "    \"Terrible customer service.\",\n",
        "    \"Totally worth the price.\",\n",
        "    \"It broke after one use.\",\n",
        "    \"Best purchase I’ve made this year.\",\n",
        "    \"I hate it so much.\",\n",
        "    \"Very satisfied with the results.\",\n",
        "    \"Not recommended at all.\"\n",
        "]\n",
        "labels = [\n",
        "    \"positive\",\n",
        "    \"negative\",\n",
        "    \"positive\",\n",
        "    \"negative\",\n",
        "    \"positive\",\n",
        "    \"negative\",\n",
        "    \"positive\",\n",
        "    \"negative\",\n",
        "    \"positive\",\n",
        "    \"negative\"\n",
        "]\n",
        "\n",
        "# Instantiate and train the recommender\n",
        "recommender = NeuralNetworkModelRecommender(data_type='text', epochs=10)\n",
        "recommender.fit((texts, labels))\n",
        "\n",
        "# Recommend the best ML model based on metadata\n",
        "result = recommender.recommend((texts, labels))\n",
        "print(\"\\nRecommendation Result:\")\n",
        "print(f\"Problem Type: {result[0]}\")\n",
        "print(f\"Best Model: {result[1]}\")\n",
        "print(f\"Scoring Metric: {result[2]}\")\n",
        "print(f\"Cross-Validation Score: {result[3]:.4f}\")"
      ],
      "metadata": {
        "id": "3wi3RsFz3dCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "accuracy_score = recommender.evaluate_recommendation_accuracy((texts, labels))\n",
        "print(f\"Recommendation Accuracy (probabilistic): {accuracy_score:.4f}\")"
      ],
      "metadata": {
        "id": "_7XhldVMUgTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on Image Dataset"
      ],
      "metadata": {
        "id": "wZgCFfYk4c8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Load original data\n",
        "digits = load_digits()\n",
        "X, y = digits.images, digits.target\n",
        "\n",
        "# Resize to 32x32 and reshape\n",
        "X_resized = np.array([resize(img, (32, 32), anti_aliasing=True) for img in X])\n",
        "X_resized = X_resized.reshape(-1, 32, 32, 1)\n",
        "\n",
        "# Verify the size of image\n",
        "print(f\"Image dataset resized: {X_resized.shape[0]} samples of shape {X_resized.shape[1:]}\")\n",
        "\n",
        "# Train as before\n",
        "recommender = NeuralNetworkModelRecommender(data_type='image', epochs=20)\n",
        "recommender.fit((X_resized, y))\n",
        "\n",
        "# Run recommendation\n",
        "problem_type, best_model, metric, score = recommender.recommend((X_resized, y))\n",
        "print(\"\\nRecommendation Result:\")\n",
        "print(f\"Problem Type: {problem_type}\")\n",
        "print(f\"Best Model: {best_model}\")\n",
        "print(f\"Scoring Metric: {metric}\")\n",
        "print(f\"Cross-Validation Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "PSLqEy2U4dHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on Tabular Data (Classification)"
      ],
      "metadata": {
        "id": "FYe0ZNwX5yzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the tabular dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Optional: Scale features for better training stability\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the recommender for tabular data\n",
        "recommender = NeuralNetworkModelRecommender(data_type='tabular', epochs=20)\n",
        "recommender.fit((X_train, y_train))\n",
        "\n",
        "# Run recommendation\n",
        "problem_type, best_model, metric, score = recommender.recommend((X_test, y_test))\n",
        "print(\"Recommendation Result:\")\n",
        "print(f\"Problem Type: {problem_type}\")\n",
        "print(f\"Best Model: {best_model}\")\n",
        "print(f\"Scoring Metric: {metric}\")\n",
        "print(f\"Cross-Validation Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "j-PGnBZU54TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "accuracy_score = recommender.evaluate_recommendation_accuracy((X_test, y_test))\n",
        "print(f\"Recommendation Accuracy (probabilistic): {accuracy_score:.4f}\")"
      ],
      "metadata": {
        "id": "CMPX632DUli7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on Tabular data (Regression)"
      ],
      "metadata": {
        "id": "cyQWUqTc7jSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# Load regression dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "sample_indices = np.random.choice(len(X_train), size=1000, replace=False)\n",
        "X_sample = X_train[sample_indices]\n",
        "y_sample = y_train[sample_indices]\n",
        "\n",
        "# Initialize and train the recommender for regression\n",
        "recommender = NeuralNetworkModelRecommender(data_type='tabular', epochs=5)\n",
        "recommender.fit((X_sample, y_sample))\n",
        "\n",
        "# Run recommendation\n",
        "problem_type, best_model, metric, score = recommender.recommend((X_test, y_test))\n",
        "print(\"\\nRecommendation Result:\")\n",
        "print(f\"Problem Type: {problem_type}\")\n",
        "print(f\"Best Model: {best_model}\")\n",
        "print(f\"Scoring Metric: {metric}\")\n",
        "print(f\"Score: {score:.4f}\")"
      ],
      "metadata": {
        "id": "4g66-0GD7oQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Notes**\n",
        "# Advantages\n",
        "- Automated scaling, normalization, feature extraction (using PCA, CNN-based feature extraction for images), and text vectorization are integrated into the pipeline\n",
        "- Multi-modal support for tabular, image, pixel, and text data.\n",
        "- Includes heuristic methods such as linear separability and linear trend checks, saving computational resources\n",
        "- Extracts key statistical metadata (e.g., mean, std, skewness, kurtosis) from the data and uses a neural network to predict the best performing algorithm\n",
        "- Diverse Algorithm Options\n",
        "- Cross-Validation for Robust Estimation\n",
        "- Clean Code Structure\n",
        "- Fast for Classification Problems\n",
        "\n",
        "# Disadvantages\n",
        "- Limited Generalization due to high dependence on heuristic methods\n",
        "- Overhead in Feature Extraction for Images\n",
        "- Limited Feature Insights\n",
        "- Fixed Underlying Model Parameters (e.g., RandomForestClassifier, GradientBoostingClassifier)\n",
        "- Potential Overfitting of the Metadata Model\n",
        "- Inconsistent Accuracy for Smaller Dataset (For  the Overall Model not Individual  Models)\n",
        "- Slow for Regression Problem Type"
      ],
      "metadata": {
        "id": "tq1i15B1-e8_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OxRwB3XGBW-P",
        "c7m93TrB3aOn",
        "wZgCFfYk4c8G",
        "FYe0ZNwX5yzu",
        "cyQWUqTc7jSX"
      ],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}